{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26650eef-4bbd-4d7e-b193-78d851a5225e",
   "metadata": {},
   "source": [
    "# Classify Publications\n",
    "\n",
    "This experiment aims to classify academic publications into specific research fields, such as Machine Learning, Computer Vision, Natural Language Processing, etc. Using a predefined set of research fields and associated keywords, each title is classified into one or more research fields. The results, including single-label and multi-label classifications with corresponding percentages, are appended as new columns to the dataset and saved as 'ClassifiedPublications.csv'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65e442aa-8cd7-4c69-8179-5d39ead85715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c10dc7fd-b404-437c-b1b4-6d10fd66b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of research fields in Tübingen (Source: CS Rankings)\n",
    "research_fields = [\n",
    "    'Computer Vision',\n",
    "    'Robotics',\n",
    "    'Bioinformatics and Computational Biology',\n",
    "    'Human-Computer Interaction',\n",
    "    'Design Automation of Microelectronic Systems (EDA)',\n",
    "    'Visualization', \n",
    "    'Web Information Retrieval',\n",
    "    'Natural Language Processing',\n",
    "    'Machine Learning', \n",
    "]\n",
    "\n",
    "# List of keywords per research field\n",
    "keywords = {\n",
    "    'Machine Learning': ['data',  'neural', 'network', 'model', 'algorithm', 'convex', 'deep', 'learning', 'gradient', 'bias', 'regression', 'probabilistic', 'random', 'gaussian', 'probability', 'newton', 'bayes', 'bregman', 'optimization', 'clustering', 'supervised', 'unsupervised', 'zero-shot', 'SGD', 'linear', 'Kullback-Leibler', 'robust', 'transformer', 'parameter', 'causal', 'feature', 'selection', 'kernel', 'distribution', 'support vector machine', 'classif', 'support measure machine', 'decision tree', 'statistical', 'regularization', 'certainty', 'attention', 'encoder', 'decoder', 'graph', 'domain', 'line search', 'generalization', 'posterior', 'likelihood', 'inference', 'adversarial', 'invariance', 'risk', 'fair', 'optimal', 'function', 'dimensionality reduction'],\n",
    "    'Computer Vision': ['vision', 'image', 'pixel', 'video', '2d', '3d', 'reconstruction', 'segmentation', 'scene', 'GAN', 'VR', 'object', 'detection', 'synthesis', 'optical flow', 'radiance', 'supervised', 'autonomous', 'driving', 'urban', 'occupancy', 'grid', 'stereo', 'camera', 'depth', 'zero-shot', 'pose estimation', 'shape estimation', 'geometric', 'curve', 'point', 'cloud', 'face', 'render', 'texture', 'neural fields', 'gaussian splatting', 'CNN', 'RGB', 'RGBD', 'clothing', 'pose', 'shape', 'people', 'registration', 'occlusion', 'geometry', 'convolution', 'photo', 'recognition', 'human shape', 'human pose', 'LiDAR'],\n",
    "    'Bioinformatics and Computational Biology': ['bio', 'bacteria', 'gene ', 'protein', 'protein sequence', 'biomolecular', 'protein structure', 'SBML', 'biosynthesis', 'disease', 'diagnosis', 'medical', 'health', 'surgeon', 'surgery', 'medicine', 'biological', 'bacterium', 'glutamicum', 'Homology', 'molecular', 'genotype', 'genom', 'drug', 'neural population', 'biomedical', 'biomedicine', 'antibody', 'cancer'],\n",
    "    'Human-Computer Interaction': ['eye', 'eye tracking', 'eye movement', 'gaze-based', 'eyelid', 'pupil', 'iris', 'gaze', 'head', 'touch', 'grip', 'haptic', 'tactile', 'contact', 'finger', 'limb', 'hug', 'engagement', 'facial videos', 'classroom', 'hand-raising', 'hand raising', 'classroom videos', 'writing', 'pupil diameter', 'digital classrooms', 'human-robot', 'human-robot-interaction', 'human gaze', 'corneal', 'eye features', 'retina', 'retinal projection', 'human activity', 'stress', 'compulsive', 'children', 'eye-based' 'human-robot collaboration', 'eye opening', 'iris gaze', 'raw eye', 'pupil segmentation', 'eye movement feature', 'eye and head tracking', ],\n",
    "    'Design Automation of Microelectronic Systems (EDA)': ['chip', 'HW', 'hardware', 'hardware accelerator', 'accelerator', 'low-power', 'embedded system', 'embedded software', 'firmware', 'timing simulation', 'scheduling', 'energey-efficient', 'RISC', 'firmware synthesis', 'firmware design', 'time synchronization', 'delay estimation', 'timing estimation', 'semiconductor', 'RTL', 'RTL simulation', 'energy-efficient', 'energy efficiency', 'energy consumption', 'programmable', 'silicon', 'energy saving', 'power optimization', 'processor', 'GPU core', 'multi-processor', 'multi-core', 'multi-processor-SoC', 'multi-core SoC'],\n",
    "    'Robotics': ['robot', 'SLAM', 'occupancy', 'grid', 'odometry', 'mobile', 'path', 'planning', 'occlusion', 'localization', 'track', 'RFID', 'navigation', 'laser', 'sensor', 'actuator'], \n",
    "    'Visualization': ['visualization', 'visualisation', 'visual analysis', 'visual analytics', 'interactive'], \n",
    "    'Web Information Retrieval': ['data mining', 'retrieval', 'web', 'crowdsourcing', 'document', 'query', 'relevance', 'retrieval', 'document-level', 'mitigation in ranking', 'dense retrieval', 'retrieval method', 'reranking', 'document embedding', 'search result', 'document reranking', 'document retrieval', 'inconsistent ranking', 'query likelihood', 'retrieval model', 'web search', 'search engine', 'deep retrieval models', 'information retrieval', 'ranking loss', 'ranking balance', 'data mining', 'crowdsourcing', 'web', 'web search', 'website', 'web site', 'search system', 'hyperlink', 'bag-of-hyperlinks', 'web retrieval', 'entity linking'],\n",
    "    'Natural Language Processing': ['text-to', 'corpus', 'language', 'summarization', 'gpt', 'bert', 'entity', 'natural language', 'NLP', 'natural language processing']\n",
    "}\n",
    "\n",
    "# Preprocess keywords\n",
    "def preprocess_keyword(keyword):\n",
    "    words = nltk.word_tokenize(keyword)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(table) for word in words]\n",
    "    words = [porter.stem(word.lower()) for word in words]\n",
    "    preprocessed_keyword = ' '.join(words)\n",
    "    if len(words) == 1 and keyword.count(' ') > 0: \n",
    "        preprocessed_keyword = preprocessed_keyword + ' ' * keyword.count(' ')\n",
    "    return preprocessed_keyword\n",
    "\n",
    "keywords = {research_field: [preprocess_keyword(word) for word in words] for research_field, words in keywords.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16ae51c0-d455-426b-bee7-09ff3f51283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Publications: 100%|███████████████████████████████████████████████████| 4066/4066 [00:07<00:00, 539.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess titles\n",
    "def preprocess_title(title):\n",
    "    words = nltk.word_tokenize(title)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(table) for word in words]\n",
    "    words = [word.lower() for word in words]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word and word not in stop_words]\n",
    "    words = [porter.stem(word) for word in words]\n",
    "    preprocessed_title = ' '.join(words)\n",
    "    return preprocessed_title\n",
    "\n",
    "# Create a new df (that will contain new columns from the classification)\n",
    "classified_titles_df = df.copy()\n",
    "\n",
    "# Create 3 new lists (columns)\n",
    "research_field_column = [] # Single label classification\n",
    "research_fields_column = [] # Multilabel classification\n",
    "research_field_percentages_column = [] # Scores (percentages) of multilabel classification\n",
    "for i, row in tqdm(classified_titles_df.iterrows(), total=len(classified_titles_df), desc='Classifying Publications'):\n",
    "    # Preprocess title\n",
    "    title = preprocess_title(str(row[\"Title\"]))\n",
    "\n",
    "    # Caculate number of keywords in title for each research field\n",
    "    row_dict = {research_field: 0 for research_field in research_fields}\n",
    "    for research_field in research_fields:\n",
    "        for keyword in keywords[research_field]:\n",
    "            if keyword in title:\n",
    "                row_dict[research_field] += 1\n",
    "\n",
    "    # Convert frequency of keywords to percentages\n",
    "    row_sum = sum(row_dict.values())\n",
    "    if row_sum != 0: \n",
    "        row_dict = {research_field: value / row_sum for research_field, value in row_dict.items()}\n",
    "\n",
    "    # Sort and select research fields with percentage > 0.33\n",
    "    row_research_fields = [research_field for research_field, value in row_dict.items() if value > 0.33]\n",
    "    row_research_fields = sorted(row_research_fields, key=lambda research_field: row_dict[research_field], reverse=True)\n",
    "    research_fields_column.append(row_research_fields)\n",
    "\n",
    "    # Add corresponding percentages\n",
    "    row_research_field_percentages = [row_dict[research_field] for research_field in row_research_fields]\n",
    "    research_field_percentages_column.append(row_research_field_percentages)\n",
    "\n",
    "    # For single label classification, select research field with highest score\n",
    "    # In case of draw, prefer more specific research fields than Machine Learning\n",
    "    if len(row_research_fields) != 0:\n",
    "        if  len(row_research_fields) > 1 and row_research_fields[0] == \"Machine Learning\" and row_dict[row_research_fields[0]] == row_dict[row_research_fields[1]]:\n",
    "            research_field_column.append(row_research_fields[1])\n",
    "        else:\n",
    "            research_field_column.append(row_research_fields[0])\n",
    "    else:\n",
    "        research_field_column.append('')\n",
    "\n",
    "# Append columns to df\n",
    "classified_titles_df[\"Research Field\"] = research_field_column\n",
    "classified_titles_df[\"Research Fields\"] = research_fields_column\n",
    "classified_titles_df[\"Research Field Percentages\"] = research_field_percentages_column\n",
    "\n",
    "# Save new dataset\n",
    "classified_titles_df.to_csv('./../dat/ClassifiedPublications.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd535f9-c670-41c9-a835-0c5bd53ee722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
